{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df15767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "from geopy.distance import geodesic\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6794fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10870717",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../../data/processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48504fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/processed/categoricals_numericals_raw.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategoricals_numericals_raw.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/processed/categoricals_numericals_raw.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(directory,\"categoricals_numericals_raw.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee9e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ef444",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637a3c1",
   "metadata": {},
   "source": [
    "# Meals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "meal_columns= ['day_' + str(i) + \"_meals\" for i in range(1, 29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_meals = set()\n",
    "\n",
    "# Iterate through each column that contains meal lists\n",
    "for col in meal_columns:\n",
    "    # Split the comma-separated values in each cell and add them to the set\n",
    "    unique_meals.update(data[col].str.split(',').explode().str.strip())\n",
    "\n",
    "unique_meals.discard(np.nan)\n",
    "# Convert the set of unique meal values to a list\n",
    "unique_meals_list = list(unique_meals)\n",
    "\n",
    "# Print the list of unique meal values\n",
    "print(unique_meals_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b70dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_meal_columns = []\n",
    "\n",
    "for meal in unique_meals:\n",
    "    data[meal] = data.apply(lambda row: any(meal in str(cell) for cell in row[meal_columns]), axis=1).astype(int)\n",
    "    new_meal_columns.append(meal)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[new_meal_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcab1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[new_meal_columns].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9845e73f",
   "metadata": {},
   "source": [
    "**High Frequency, Low Uniqueness:** Columns like \"Breakfast,\" \"Dinner,\" and \"Lunch\" occur very frequently but are not very unique, meaning many tours will have these features, and thus, they may not significantly contribute to differentiating between tours. However, they can still be essential for filtering; for example, if a user specifically wants a tour that includes breakfast.\n",
    "\n",
    "**Low Frequency, High Uniqueness:** Columns like \"Cooking Class,\" \"Morning tea,\" and \"Light refreshments on the river cruise\" are unique but occur very infrequently. These can be valuable for differentiation but may apply to very few cases, limiting their overall impact on the recommender.\n",
    "\n",
    "**Moderate Frequency, Moderate Uniqueness:** Columns like \"Dinner with Wine,\" \"Welcome Reception,\" and \"Be My Guest\" can be the most beneficial as they offer a balance between uniqueness and frequency, allowing them to significantly contribute to the recommendation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list.extend(meal_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aad6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list.extend(['Morning tea', 'Dinner and local drinks (including wine and spirits)', 'Light refreshments on the river cruise', 'Dinner (one evening only)', 'Dinner (one evening only)','Cooking Class' ])\n",
    "drop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb495d1",
   "metadata": {},
   "source": [
    "# Accommodation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d86004",
   "metadata": {},
   "outputs": [],
   "source": [
    "itineraries = pd.read_csv('../data/processed/itineraries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab625307",
   "metadata": {},
   "outputs": [],
   "source": [
    "itineraries['accommodation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "itineraries['accommodation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999df0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "itineraries[itineraries['accommodation'] == \"Hilton Vienna Park.\"]['tour_option_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = itineraries['accommodation'].dropna().value_counts(dropna=False)\n",
    "(value_counts > 1).sum(), (value_counts > 2).sum(), (value_counts > 3).sum(), (value_counts > 5).sum(), (value_counts > 10).sum(), (value_counts > 15).sum(), (value_counts > 20).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values in the 'accommodation' column that occur more than 15 times\n",
    "new_accommodation_columns = value_counts[value_counts > 15].index.tolist()\n",
    "new_accommodation_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_snake_case(input_string):\n",
    "    # Convert to snake case\n",
    "    snake_case_string = '_'.join(word.lower() for word in input_string.split())\n",
    "    \n",
    "    # Replace characters with '_'\n",
    "    cleaned_string = re.sub(r'['',.\\-()/]', '', snake_case_string)\n",
    "\n",
    "    # Truncate to a maximum of 40 characters\n",
    "    if len(cleaned_string) > 40:\n",
    "        cleaned_string = cleaned_string[:40]\n",
    "    \n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31bb4e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_accommodation_columns_snake = list(map(convert_to_snake_case, new_accommodation_columns))\n",
    "new_accommodation_columns_snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accommodation_columns= ['day_' + str(i) + \"_accommodation\" for i in range(1, 29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560825fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns_df = pd.DataFrame()\n",
    "\n",
    "for accommodation in new_accommodation_columns_snake:\n",
    "    new_columns_df[accommodation] = data.apply(\n",
    "        lambda row: any(accommodation in convert_to_snake_case(str(cell))\n",
    "                        for cell in row[accommodation_columns]), axis=1).astype(int)\n",
    "\n",
    "data = pd.concat([data, new_columns_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a170df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[new_accommodation_columns_snake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_accommodation_columns_snake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c476bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[new_accommodation_columns_snake].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list.extend(accommodation_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(drop_list, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020c2cb",
   "metadata": {},
   "source": [
    "# Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['locations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc072bf",
   "metadata": {},
   "source": [
    "## Data Transformation Plan\n",
    "\n",
    "1. **Number of Locations**\n",
    "   - **Objective**: Create a new feature representing the number of locations visited during the tour.\n",
    "\n",
    "2. **Unique Locations**\n",
    "   - **Objective**: Extract unique locations visited during the tour and represent them as binary features using one-hot encoding.\n",
    "   - **Example**: Create binary columns like `visited_Anchorage`, `visited_Valdez`, etc.\n",
    "\n",
    "3. **Country Code**\n",
    "   - **Objective**: Extract unique country codes visited during the tour and represent them as binary features using one-hot encoding.\n",
    "   - **Example**: Create binary columns like `visited_country_US`, `visited_country_CA`, etc.\n",
    "\n",
    "4. **Latitude and Longitude Statistics**\n",
    "   - **Objective**: Calculate statistics for latitude and longitude across all locations to provide insights into the central tendencies of the tour locations.\n",
    "   - **Statistics to Calculate**:\n",
    "     - Mean latitude and longitude\n",
    "     - Median latitude and longitude\n",
    "     - Minimum latitude and longitude\n",
    "     - Maximum latitude and longitude\n",
    "     - Range of latitude and longitude\n",
    "\n",
    "5. **Distance Traveled**\n",
    "   - **Objective**: Calculate the total distance traveled during the tour by summing the distances between consecutive locations.\n",
    "   - **Method**: Use the Haversine formula to calculate the distance between two latitude-longitude points.\n",
    "\n",
    "By following this plan, we aim to enhance the understanding of tour data by creating meaningful features and statistics related to the locations visited during the tour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d8847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/processed/locations_visited.csv'\n",
    "locations_visited = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d63ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_with_tour_option_id = pd.merge(\n",
    "    locations_visited, \n",
    "    itineraries[['id', 'tour_option_id']].rename(columns={'id': 'itinerary_id'}), \n",
    "    on='itinerary_id', how='left')\n",
    "locations_with_tour_option_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30379bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "def calculate_total_distance(locations_df):\n",
    "    locations = locations_df.to_dict('records')  # Convert DataFrame to a list of dicts\n",
    "    total_distance = 0\n",
    "    for i in range(1, len(locations)):\n",
    "        lat1, lon1 = locations[i-1]['latitude'], locations[i-1]['longitude']\n",
    "        lat2, lon2 = locations[i]['latitude'], locations[i]['longitude']\n",
    "        \n",
    "        if isnan(lat1) or isnan(lon1) or isnan(lat2) or isnan(lon2):\n",
    "            continue  # Skip if any of the coordinates are NaN\n",
    "        \n",
    "        coord1 = (lat1, lon1)\n",
    "        coord2 = (lat2, lon2)\n",
    "        total_distance += geodesic(coord1, coord2).miles\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_with_tour_option_id.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2adb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_with_tour_option_id['countryCode'].fillna('', inplace=True)\n",
    "locations_with_tour_option_id['longitude'].fillna(0, inplace=True)\n",
    "locations_with_tour_option_id['latitude'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the union_sets function to 'unique_locations' and 'unique_country_codes'\n",
    "aggregated_data = locations_with_tour_option_id.groupby('tour_option_id').agg({\n",
    "    'name': [('count', 'count'), ('nunique', 'nunique'), ('location', lambda x: list(set(x)))],\n",
    "    'countryCode': [('count', 'count'), ('nunique', 'nunique'), ('codes', lambda x: list(set(x)))],\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean'\n",
    "    \n",
    "}).reset_index()\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the MultiIndex column names\n",
    "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e17828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total distance traveled for each tour_option_id\n",
    "aggregated_data['total_distance_traveled'] = locations_with_tour_option_id.groupby('tour_option_id').apply(calculate_total_distance).reset_index(name='total_distance_traveled')['total_distance_traveled']\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_country_codes = locations_with_tour_option_id['countryCode'].unique()\n",
    "unique_country_codes = np.delete(unique_country_codes, np.where(unique_country_codes == ''))\n",
    "unique_country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for each unique country code and set values\n",
    "for country_code in unique_country_codes:\n",
    "    # Initialize the column with zeros\n",
    "    aggregated_data[country_code] = 0\n",
    "    \n",
    "    # Set 1 if the country code is in the countryCode_codes column for that row\n",
    "    for index, row in aggregated_data.iterrows():\n",
    "        if country_code in row['countryCode_codes']:\n",
    "            aggregated_data.at[index, country_code] = 1\n",
    "\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lat_long = aggregated_data[['latitude_mean', 'longitude_mean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a39ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_data = aggregated_data.drop('countryCode_codes', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts = locations_with_tour_option_id['name'].value_counts()\n",
    "unique_locations = name_counts[name_counts > 15].index.tolist()\n",
    "unique_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3863a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.DataFrame(0, index=aggregated_data.index, columns=unique_locations)\n",
    "locations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eda993",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df['name_location'] = aggregated_data['name_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc58940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in unique_locations:\n",
    "    for index, row in locations_df.iterrows():\n",
    "        if location in row['name_location']:\n",
    "            locations_df.at[index, location] = 1\n",
    "\n",
    "locations_df = locations_df.drop('name_location', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432948ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = pd.concat([aggregated_data, locations_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = aggregated_data.drop('name_location', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ad7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, aggregated_data, on='tour_option_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c24b8",
   "metadata": {},
   "source": [
    "# NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of NaN values in each column\n",
    "nan_counts = data.isna().sum()\n",
    "\n",
    "# Filter and print columns that have NaN values with the count\n",
    "for column, nan_count in nan_counts.items():\n",
    "    if nan_count > 0:\n",
    "        print(f\"{column}: {nan_count} NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f02beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['sourceTourOptionName', 'maxPax'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52198781",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['countryCode_codes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_country_codes(row):\n",
    "    # If startLocationCountryCode is NaN, fill with the first country in countryCode_codes\n",
    "    if pd.isna(row['startLocationCountryCode']) and row['countryCode_codes']:\n",
    "        row['startLocationCountryCode'] = row['countryCode_codes'][0]\n",
    "    \n",
    "    # If endLocationCountryCode is NaN, fill with the last country in countryCode_codes\n",
    "    if pd.isna(row['endLocationCountryCode']) and row['countryCode_codes']:\n",
    "        row['endLocationCountryCode'] = row['countryCode_codes'][-1]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9852048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.apply(fillna_with_country_codes, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['startLocationCountryCode'].isna().sum(), data['endLocationCountryCode'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48220967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna_with_mean(row):\n",
    "    if pd.isna(row['startLocationLongitude']) and not pd.isna(row['longitude_mean']):\n",
    "        row['startLocationLongitude'] = row['longitude_mean']\n",
    "        \n",
    "    if pd.isna(row['startLocationLatitude']) and not pd.isna(row['latitude_mean']):\n",
    "        row['startLocationLatitude'] = row['latitude_mean']\n",
    "        \n",
    "    if pd.isna(row['endLocationLongitude']) and not pd.isna(row['longitude_mean']):\n",
    "        row['endLocationLongitude'] = row['longitude_mean']\n",
    "        \n",
    "    if pd.isna(row['endLocationLatitude']) and not pd.isna(row['latitude_mean']):\n",
    "        row['endLocationLatitude'] = row['latitude_mean']\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce917ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.apply(fillna_with_mean, axis=1)\n",
    "data['startLocationLongitude'].isna().sum(), data['startLocationLatitude'].isna().sum(), data['endLocationLongitude'].isna().sum(), data['endLocationLatitude'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108bdc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['activityLevel'].fillna('not_specified', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af23ba20",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d177016",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour_ids = data[['tour_option_id', 'tour_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_2 = ['tour_id', 'tour_option_id', 'fkSeasonId', 'tour_id.1', 'countries_visited', 'locations', 'countryCode_codes', 'isPrivateRequest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa57ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_one_hot = ['productType', 'brand', 'activityLevel',\n",
    "       'lowestOptionRoomType', 'startLocationName', \n",
    "       'startLocationCountryCode', 'endLocationName',\n",
    "       'endLocationCountryCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39025d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(drop_list_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b94167",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(data, columns=to_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d144d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb3c150",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32416103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics for each column\n",
    "descriptive_stats = data_encoded.describe()\n",
    "\n",
    "# Identify columns that are not in [0, 1] range\n",
    "columns_not_in_range = []\n",
    "for column in descriptive_stats.columns:\n",
    "    min_value = descriptive_stats.at['min', column]\n",
    "    max_value = descriptive_stats.at['max', column]\n",
    "    if min_value < 0 or max_value > 1:\n",
    "        columns_not_in_range.append(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c39f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_not_in_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features not in [0, 1] range\n",
    "data_encoded[columns_not_in_range] = scaler.fit_transform(data_encoded[columns_not_in_range])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a294cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d09b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_snake_case(name):\n",
    "    # Replace all non-alphanumeric characters with underscores\n",
    "    s = re.sub('[\\W_]+', '_', name)\n",
    "    # Convert to lowercase\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [convert_to_snake_case(col) for col in data_encoded.columns]\n",
    "data_encoded.columns = new_columns\n",
    "data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ab9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(directory, f\"categoricals_numericals_encoded_scaled.csv\")\n",
    "data_encoded.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(directory, f\"reference_tour_ids.csv\")\n",
    "tour_ids.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(directory, f\"mean_lat_long.csv\")\n",
    "mean_lat_long.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
